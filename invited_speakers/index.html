













<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Dustin Tran, edited by Chris Cremer">
  <link rel="shortcut icon" href="../img/favicon.ico" type="image/x-icon">
  <title>Invertible Neural Nets and Normalizing Flows - Call for Papers</title>

  <!-- CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/css/bootstrap.min.css">
  <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="../css/main.css">

  <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->


</head>

<body>
  <!-- <div class="container" style="margin-left:20px"> -->
  <div class="container">

    <div class="row" style="padding:20px">

      <div class="col-xs-3 col-sm-3 col-md-3">
        <img src="../img/innf_logo.gif" style="height:70px" hspace="50"></img>
      </div>

      <div class="col-xs-12 col-sm-12 col-md-9">
        <div class="row" style="margin-bottom:-10px;">
          <div class="col-xs-12 hidden-sm hidden-md hidden-lg">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <div class="hidden-xs">
            <!--
            <a href="http://nips.cc">
              <img src="/img/nips.svg" class="pull-right" style="margin:0px -90px
              0 0; height:140px; width:250px;">
            </a>
            -->
          </div>
          <h2>Workshop on Invertible Neural Nets and Normalizing Flows</h2>
          <br>
          <!--           <p class="lead">
            December 2, 2018<br>
            <a href="https://goo.gl/maps/GZ2CkVfpFhL2">Le 1000 Conference Center</a><br>
            <a href="https://goo.gl/maps/GZ2CkVfpFhL2">1000 Rue de la Gauchetière Ouest</a><br>
            <a href="https://goo.gl/maps/GZ2CkVfpFhL2">Montréal, QC H3B 0A2, Canada</a><br>
          </p> -->
        </div>
      </div>

      <div class="col-xs-12 col-sm-3 col-md-3" id="sidebar" role="navigation">
        <hr>
        <ul class="nav nav-pills nav-stacked">
          <li><a href="../index.html">Home</a></li>
          <li><a href="../schedule/index.html">Schedule</a></li>
          <li><a href="../call/index.html">Call for Papers</a></li>
          <li><a href="../author_instructions/index.html">Author Instructions</a></li>
          <li><a href="../accepted_papers/index.html">Accepted Papers</a></li>
          <li><a href="./index.html">Invited Speakers</a></li>
        </ul>
        <hr>
        <ul class="nav temp">
          <li style="padding:0px 15px 5px;">Organizers</li>
          <li><a href="https://chinweihuang.com/">Chin-Wei Huang</a></li>
          <li><a href="https://mila.quebec/en/person/david-scott-krueger/">David Krueger</a></li>
          <li><a href="https://riannevdberg.github.io/">Rianne van den Berg</a></li>
          <li><a href="https://homepages.inf.ed.ac.uk/s1459647/">George Papamakarios</a></li>
          <li><a href="https://aidangomez.ca/">Aidan Gomez</a></li>
          <li><a href="https://chriscremer.bitbucket.io/">Chris Cremer</a></li>
          <li><a href="http://rtqichen.com">Ricky Chen</a></li>
          <li><a href="https://aaroncourville.wordpress.com/">Aaron Courville</a></li>
          <li><a href="https://danilorezende.com/">Danilo Rezende</a></li>
        </ul>
      </div>

      <div class="col-xs-12 col-sm-9 col-md-9">
        <hr>
        <div class="row">


            
            <h2>Invited Speakers</h2>
            <hr>
            
<!-- 


            <h3>Laurent Dinh</h3>
            <h4>Building a Tractable Generator Network</h4> 
            <br>
            <strong>Bio:</strong> Laurent Dinh is a research scientist at Google Brain Montréal. His research focus has been on deep generative models, probabilistic modeling, and generalization in deep learning. He's best known for his contribution in normalizing flows generative models, such as NICE and Real NVP, and questioning the conjecture on the relationship between flatness and generalization. 
            He obtained his PhD in deep learning at Mila, under the supervision of Yoshua Bengio, during which he visited Google Brain and DeepMind. Before that, he graduated from École Centrale Paris in Applied Mathematics and from École Normale Supérieure de Cachan in machine learning and computer vision.
            <br>
            <hr>


            <h3>Diederik Kingma and Prafulla Dhariwal</h3>
            <hr> -->




            <h3><a href="http://evjang.com/">Eric Jang</a> </h3>
            <h4>Normalizing Flows: A Tutorial</h4> 
            An introductory tutorial on Normalizing Flows, why they are awesome, and how to get started with researching this family of models. We will cover how to implement basic flows in JAX, discuss recent breakthroughs, and share practical tips for training and evaluating these models. Finally, we will discuss open challenges in the field, and implications of flow-based models ML for AI hardware and software. There will be live Colab notebook coding, feel free to bring a laptop if you'd like to follow along.
            <br>   
            <br>   
            <strong>Bio:</strong> Eric is a research engineer at Google AI (Brain Team), working on robotic grasping and manipulation. He is interested in meta-learning for robotics, deep generative models, and Artificial Life. He received his M.Sc. in CS and Bachelors in Math/CS at Brown University in 2016.
            <br>
            <hr>      



            <h3><a href="https://jmtomczak.github.io/">Jakub Tomczak</a> </h3>
            <h4>Householder meets Sylvester: Normalizing flows for variational inference</h4> 
            Stochastic variational inference allows for posterior inference in increasingly large and complex problems using stochastic gradient ascent. However, despite its many success, it has drawbacks compared to other inference methods such as MCMC. Variational inference searches for the best posterior approximation within a parametric family of distributions, and, thus, the true posterior distribution can only be recovered exactly if it happens to be in the chosen family. In particular, with widely used simple variational families such as diagonal covariance Gaussian distributions, the variational approximation is likely to be insufficient. Therefore, designing tractable and more expressive variational families is an important problem.
            <br>
            Recently, a general framework for constructing more flexible variational distributions, called normalizing flows, was proposed. The idea of normalizing flows is to transform a base (simple) density through a number of invertible parametric transformations with tractable Jacobians into more complicated distributions. In this talk, I will present two families of normalizing flows to improve the variational inference. The first one is a volume-preserving flow based on Householder transformations that allows to efficiently parameterize variational posteriors. Next, I will show non-linear normalizing flows based on the Sylvester determinant lemma and QR-decomposition.
            <br>   
            <br>   
            <strong>Bio:</strong> Jakub Tomczak is a deep learning research engineer at Qualcomm AI Research since October 2018. Before, he was a postdoc (Marie Sklodowska-Curie Individual-Fellow) in Amsterdam Machine Learning Lab (AMLAB) at the University of Amsterdam under Prof. Max Welling supervision, from Oct 2016 to Sept 2018. He has received Ph.D. in machine learning (with honors) from Wroclaw University of Technology (Poland) in March 2013. After Ph.D. studies he was a postdoc and an assistant professor therein, and he worked on ensemble learning, probabilistic modeling and deep learning (with a special interest in Boltzmann machines) applied to credit scoring, medicine (clinical data) and image analysis. Recently, his research is focused on deep generative modeling (Variational Auto-Encoders) for medical imaging and image analysis.
            <br>
            <hr>         




            <h3><a href="http://www.jessebett.com/">Jesse Bettencourt</a> </h3>
            <h4>Neural Ordinary Differential Equations for Continuous Normalizing Flows</h4> 
            This talk will review a family of continuous-depth neural network models, highlighting their properties, such as invertibility, which make them well-suited to normalizing flows. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of hidden state with a neural network. This instantaneous change in hidden state along with an initial state given by model inputs, such as data, defines an initial value problem. The output of the model is computed by solving the initial value problem, integrating the derivative, with a numerical solver. 
            These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for computational cost. Another significant benefit is that the model can be inverted by integrating the parameterized dynamics backwards in time. 
            In addition to parameterizing and integrating the change in hidden state we can also track the corresponding instantaneous change in probability density, which we call Continuous Normalizing Flows. This results in an invertible generative model with unbiased density estimation and one-pass sampling. Further, by estimating terms in the change of density we can scale to high-dimensional data without restricting model architecture in a method called FFJORD.
            <br>   
            <br>   
            <strong>Bio:</strong> Jesse Bettencourt is a graduate student in Machine Learning at the University of Toronto and the Vector Institute supervised by Drs. David Duvenaud and Roger Grosse. He is currently pursuing follow-up research on Neural Ordinary Differential Equations, and is generally interested in approximate inference for latent variable models. He also teaches Probabilistic Learning and Reasoning.
            <br>
            <hr>    



            <h3> <a href="https://jhjacobsen.github.io/">Jorn-Henrik Jacobsen</a> </h3>  
            <h4>Invertible Neural Networks for Understanding and Controlling Learned Representations</h4> 
            One way to understand deep networks is to analyze the information they discard about the input from layer to layer. However, estimating mutual information between input and hidden representations is intractable in high dimensional problems. Invertible deep networks circumvent this problem by guaranteeing information preservation. In this talk, I will discuss surprising similarities between non-invertible and invertible deep networks. Further, I will discuss how invertible models give rise to an alternative viewpoint on adversarial examples. Under this viewpoint adversarial examples are a consequence of excessive invariances learned by the classifier, manifesting themselves in striking failures when evaluating the model on out of distribution inputs. I will discuss how the commonly used cross-entropy objective encourages such overly invariant representations. Finally, I will present an extension to cross-entropy that, by exploiting properties of invertible deep networks, enables control of erroneous invariances in theory and practice.
            <br>   
            <br>   
            <strong>Bio:</strong> Dr. Jörn Jacobsen is a postdoc at Vector Institute and University of Toronto, supervised by Richard Zemel and collaborating with faculty members David Duvenaud and Roger Grosse. Previously, he was a postdoc in the lab of Matthias Bethge in Tübingen. Before, he did his Ph.D. at the University of Amsterdam under supervision of Arnold Smeulders. Currently, he mainly works on invertible networks, generative modeling, out of distribution generalization and re-purposing adversarial examples for understanding learned representations.
            <br>
            <hr>    



        </div>

        <footer>
          &nbsp;
        </footer>

      </div>

      <!-- JavaScript -->
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
      <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.1/js/bootstrap.min.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
      </script>
      <script type="text/javascript" src="../js/main.js"></script>
</body>

</html>
























